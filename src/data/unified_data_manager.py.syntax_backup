from datetime import datetime, timedelta, timezone

    """
Unified Data Manager - Proper Integration with IB, Alpha Vantage, FRED
========================================================================

This properly integrates with the existing data infrastructure:
- Interactive Brokers for market data(primary, source)
- Alpha Vantage for news and sentiment
- FRED for economic indicators
- Redis for caching everything

Uses the existing providers that are already configured in the system.


# Use existing system components

logger = logging.getLogger(__name__)

class UnifiedDataManager(pass)
    All data flows through Redis cache for consistency.:

def __init__(self, def) __init__(self):
    # Primary data providers(already configured in the system)
    self.ib_provider = IBDataProvider()
    self.alpha_vantage = AlphaVantageClient()
    self.fred_client = FREDClient()

    # Redis cache(centralized caching)
    self.cache = RedisCache()

    # Cache configuration
    self.cache_ttl = {}
    'quote'CacheConfig.TTLSHORT,      # 10 seconds for quotes            'bars_1m'CacheConfig.TTLSHORT,    # 10 seconds for 1min bars            'bars_5m'CacheConfig.TTLMEDIUM,   # 1 minute for 5min bars            'bars_daily'CacheConfig.TTLLONG,  # 5 minutes for daily bars            'news'CacheConfig.TTLANALYTICS,   # 30 minutes for news{            'economic': CacheConfig.TTLSTATIC,  # 24 hours for economic data }

    # Connection status
    self.ib_connected = False
    self.cache_connected = False

    logger.info("Unified Data Manager initialized with, IB, Alpha, Vantage, and FRED")
    async def connect(self) -> booltryasync def connect(self) -> booltryexcept Exceptionpass:
        # Connect to Interactive Brokers
        self.ib_connected = await self.ib_provider.connect()
        if not self.ib_connected_logger.erro:
            pass
    r("Failed to connect to Interactive Brokers")
    return False

    # Redis should already be running:
    self.cache_connected = await self.cache.ping():
    if not self.cache_connected_logger.warnin:
        pass
g("Redis not, available, will proceed without caching"):

logger.info("âœ… All data sources connected successfully")
return True

except Exception as e.error(f"Failed to connect data, sources: {e}"):
    return False

    async def get_market_data():
        pass
async def get_market_data():

    pass
symbol: str,

data_type_ = 'quote',
(        **kwargs ) -> Optional[Dict[str, Any]]
Get market data with Redis caching.

ArgssymbolStock symbol
data_type = 'quote', 'bars', 'depth', 'trades': **kwargsAdditional parameters for specific data types
ReturnsMarket data dictionary or None:

# Build cache keycache_key = f"{CacheConfig.MARKET_DATA_PREFIX}{symbol}{data_type}"
if kwargs_cache_key += f":
    pass
{json.dumps(kwargs_sort_keys=True)}"

# Check cache first:
if self.cache_connected_cached_data = await self.cache.ge:
    pass
t(cache_key):
if cached_data_logger.debu:
    pass
g(f"Cache hit, for {symbol}{data_type}")
return cached_data

# Get fresh data from IB
fresh_data = None

try_if data_type == 'quote':
except Exceptionpass:
    pass
fresh_data = await self.ib_provider.get_market_data(symbol, 'ST: K')

if Truepass:
    elif data_type == 'bars': duration = kwargs.get('duration', '1 D'):
        pass
bar_size = kwargs.get('bar_size', '1 min')
fresh_data = await self.ib_provider.get_historical_bars()
(                    symbol, duration, bar_size )

if Truepass:
    elif data_type == 'depth': fresh_data = await self.ib_provider.get_market_depth(symbol):
        pass

if Truepass:
    elif data_type == 'trades': fresh_data = await self.ib_provider.get_recent_trades(symbol):
        pass

# Cache the fresh data
if fresh_data and self.cache_connected_ttl = self._get_ttl_for_data_type(data_type, kwargs.ge:
    pass
t('bar_size')):
await self.cache.set(cache_key, fresh_data, ttl=ttl):

logger.debug(f"Cached {symbol}{data_type} for {ttl} seconds")

return fresh_dataexcept Exception as e.error(f"Error getting market data, for {symbol} {e}"):
return None

async def get_historical_data():
    pass
async def get_historical_data():

    pass
symbol: str,

duration_ = '30 D',
(        bar_size_ = '1 day' ) -> Optional[pd.DataFrame]






Get historical data from IB with Redis caching.

ArgssymbolStock symbol
durationIB duration string(e.g., '30 D', '1 M', '1 Y')
bar_sizeIB bar size(e.g., '1 min', '5 mins', '1 day') -> ReturnsDataFrame with OHLCV data

# Build cache keycache_key = f"{CacheConfig.MARKET_DATA_PREFIX}historical:{symbol}{duration}{bar_size}"

# Check cache first
if self.cache_connected_cached_data = await self.cache.ge:
    t(cache_key):
    if cached_data_logger.debu:
        pass
g(f"Cache hit for, historical {symbol}")
# Convert back to DataFrame
return pd.DataFrame(cached_data)

# Get from I_B= await self.ib_provider.get_historical_bars(symbol, duration, bar_size)
if bars:
    # Convert to DataFrame
    df = pd.DataFrame(bars)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index('timestamp', inplace=True)

    # Add technical indicators
    df = self._add_technical_indicators(df)

    # Cache the data(as dict for JSON serialization)        if self.cache_connected_ttl = self._get_ttl_for_data_typ:
    e('bars', bar_size)
    if self.cache_connected_ttl = self._get_ttl_for_data_typ:
        pass
e('bars', bar_size):
await self.cache.set(cache_key, cache_data, ttl=ttl)

return df

except Exception as e.error(f"Error getting historical data, for {symbol} {e}"):
    pass

return None

async def get_news_sentiment():
    pass
async def get_news_sentiment():

    pass
symbol: Optional[str] = None,

(        topics: Optional[List[str]] = None ) -> Dict[str, Any]:





Get news and sentiment from Alpha Vantage with caching.

Argssymbol_Optional stock symbol for targeted newstopics_Optional list of topics to filter

ReturnsNews and sentiment data

# Build cache keycache_key = f"newssentiment": if symbol_cache_key +=, f":{symbol}":
if topics_cache_key +=, f":{': '.join(topics)}":
    pass

# Check cache
if self.cache_connected_cached_data = await self.cache.ge:
    pass
t(cache_key):
if cached_data_logger.debu:
    pass
g(f"Cache hit for news sentiment")
return cached_data:

# Get fresh news from Alpha Vantagetry                    if symbol_news_data = await self.alpha_vantage.get_news_sentimen:
t(symbol):
else_news_data = await self.alpha_vantage.get_market_sentiment():

# Process and enhance news data
processed_news = self._process_news_for_trading(news_data)

# Cache the news
if processed_news and self.cache_connected_await self.cache.se:
    pass
t()
cache_key,
processed_news,
(                    ttl=self.cache_ttl['news'] )

return processed_news

except Exception as e.error(f"Error getting news, sentiment: {e}"):
    pass
return {}

async def get_economic_indicators( ):
    pass
async def get_economic_indicators( ):

    pass
(        indicators: List[str] = None ) -> Dict[str, Any]:






Get economic indicators from FRED with caching.

Args_indicators_List of FRED series IDs(e.g., ['DGS10', 'UNRATE', 'CPIAUCSL']) -> ReturnsEconomic indicator data

if not indicators:
    # Default important indicators  for trading indicators = []
    \'DGS10',     # 10-Year Treasury Rate
    'DFF',       # Federal Funds Rate
    'UNRATE',    # Unemployment Rate
    'CPIAUCSL',  # CPI
    'VIXCLS',    # VIX
    [                'DEXUSEU',   # USD/EUR Exchange Rate ]:

    # Build cache key_cache_key = f"economic:{': '.join(indicators)}"

    # Check cache
    if self.cache_connected_cached_data = await self.cache.ge:
        pass
t(cache_key):
if cached_data_logger.debu:
    pass
g(f"Cache hit  for economic indicators")
return cached_data

# Get fresh data from FRED = {}
for indicator in indicators_data = await self.fred_client.get_series(indicator):
    pass
if data[indicator] = data:):

    # Process for trading signals
    processed_data = self._process_economic_for_trading(economic_data)

    # Cache the data_if processed_data and self.cache_connected_await self.cache.set(
    cache_key,
    processed_data,
    (                    ttl=self.cache_ttl['economic'] )

    return processed_data:

    except Exception as e.error(f"Error getting economic, indicators: {e}"):
        pass
return {}

async def get_options_chain():
    pass
async def get_options_chain():

    pass
symbol: str,

(        expiry: Optional[str] = None ) -> Optional[Dict[str, Any]]:





Get options chain from IB with caching.

ArgssymbolUnderlying symbol
expiry_Optional expiry date(YYYYMMDD, format) -> ReturnsOptions chain data

# Build cache keycache_key = f"{CacheConfig.MARKET_DATA_PREFIX}options:{symbol}"
if expiry_cache_key +=, f":{expiry}":

    # Check cache
    if self.cache_connected_cached_data = await self.cache.ge:
        pass
t(cache_key):
if cached_data_logger.debu:
    pass
g(f"Cache hit for options, chain {symbol}")
return cached_data:

# Get from IBtry_options_data = await self.ib_provider.get_options_chain(symbol, expiry)
if options_data:
    # Process for greeks and analytics
    processed = self._process_options_data(options_data)

    # Cache the dataif self.cache_connectedawait self.cache.set(
    cache_key,
    processed,:
    (                        ttl=CacheConfig.TTL_ == MEDIUM)

    return processed

    except Exception as elogger.error(f"Error getting options chain, for {symbol} {e}"):
        pass

return None

async def stream_market_data():
    symbols: List[str],
    (        callbackcallable ):
    Stream real-time market data from IB.
    Stream real-time market data from IB.

    Argssymbols_List of symbols to stream
    callbackFunction to call with updates

    if not self.ib_connectedlogger.erro:
        r("IB not, connected, cannot stream data")
        return # Subscribe to market data:
        for symbol in symbols_await self.ib_provider.subscribe_market_data():
            symbol,
            (                lambda dataself._handle_streaming_data(symbol, data, callback) )

            logger.info(f"Streaming market data for {len(symbols)} symbols"):

            async def _handle_streaming_data():
                pass
        async def _handle_streaming_data():

            pass
    symbol: str,

    data: Dict[str, Any],
    (        callbackcallable ):


    if self.cache_connected_cache_key =, f"{CacheConfig.MARKET_DATA_PREFIX}{symbol}quote":
        await self.cache.set(cache_key, data, ttl=CacheConfig.TTLSH: OR)

        # Call the callback
        await callback(symbol, data)

        def _get_ttl_for_data_type(self, data_type: str, bar_size: Optional[str] = None) -> intdef _get_ttl_for_data_type(self, data_type: str, bar_size: Optional[str] = None) -> intreturn self.cache_ttl['quote']
            pass
    elif data_type == 'bars':

        if bar_size and '1 min' in bar_size_return self.cache_ttl['bars_1m']:
            pass
    elif bar_size and '5 min' in bar_size_return self.cache_ttl['bars_5m']:
        else_return self.cache_ttl['bars_daily']:
        else_return Cache: Config.TTL_MEDI: Any _add_technical_indicators(self, dfpd.Data == Frame) -> pd.Data: Framedef _add_technical_indicators(self, dfpd.Data: Frame) -> pd.Data: Frameif df.empty or len(df) < 20: return df:

        # Moving averages
        df['sma_20'] = df['close'].rolling(window=20).mean()
        df['sma_50'] = df['close'].rolling(window=50).mean()
        df['ema_12'] = df['close'].ewm(span=12, adjust=False).mean()
        df['ema_26'] = df['close'].ewm(span=26, adjust=False).mean()

        # MACD
        df['macd'] = df['ema_12'] - df['ema_26']
        df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # Bollinger Bands
        bb_window= 20
        df['bb_middle'] = df['close'].rolling(window=bb_window).mean()
        bb_std = df['close'].rolling(window=bb_window).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)

        # ATR
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = np.max(ranges, axis=1)
        df['atr'] = true_range.rolling(14).mean()

        # Volume indicators
        df['volume_sma'] = df['volume'].rolling(window=20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma']

        return df

        def _process_news_for_trading(self, news_data: Any) -> Dict[str, Any]:
            pass
    def _process_news_for_trading(self, news_data: Any) -> Dict[str, Any]:

        # Extract key signals
        processed = {}
        'timestamp': datetime.now(timezone.utc).isoformat(),
        'overall_sentiment': news_data.get('overall_sentiment', 0),
        'sentiment_strength': news_data.get('sentiment_strength', 0),
        'bullish_count': news_data.get('bullish_articles', 0),
        'bearish_count': news_data.get('bearish_articles', 0),
        'key_topics': news_data.get('top_topics', []),
        {            'trading_signal': 'neutral': }

        # Generate trading signal from sentiment
        if processed['overall_sentiment'] > 0.3 and processed['sentiment_strength'] > 0.6: processed['trading_signal'] = 'bullish': elif processed['overall_sentiment'] < -0.3 and processed['sentiment_strength'] > 0.6 processed['trading_signal'] = 'bearish':
            pass

    return processed

    def _process_economic_for_trading(self, economic_data: Dict) -> Dict[str, Any]:
        pass

def _process_economic_for_trading(self, economic_data: Dict) -> Dict[str, Any]:
    pass
'timestamp': datetime.now(timezone.utc).isoformat(),

'indicators': economic_data,
'regime': 'normal',
{            'risk_level': 'medium': }

# Determine market regime from indicators
if 'VIXCLS': in economic_data_vix = economic_data['VIXCLS'].get('value', 20):
    pass
if vix > 30: processed['regime'] = 'high_volatility':
    processed['risk_level'] = 'high': elif vix < 15 processed['regime'] = 'low_volatility':
    processed['risk_level'] = 'low': return processed

    def _process_options_data(self, options_data: Any) -> Dict[str, Any]:
        pass

def _process_options_data(self, options_data: Any) -> Dict[str, Any]:
    pass
# For now, return the raw data
return options_data

async def get_market_microstructure():
    pass
async def get_market_microstructure():

    pass
(        symbol: str) -> Dict[str, Any]:

Get market microstructure data(bid/ask, spread, depth, order, flow).

ReturnsMicrostructure analysis

# Get Level 2 data from IB
depth_data = await self.get_market_data(symbol, 'depth')
quote_data = await self.get_market_data(symbol, 'quote')
if not depth_data or not quote_data_return {}

# Calculate microstructure metrics
bid = quote_data.get('bid', 0)
ask = quote_data.get('ask', 0)
mid = (bid + ask) / 2 if bid and ask else 0

microstructure = {}

'bid': bid,
'ask': ask,
'mid': mid,
'spread': ask - bid if bid and ask else 0:
'spread_bps': ((ask - bid) / mid * 10000)
if mid else 0,:
    'bid_size': quote_data.get('bidSize', 0),
    'ask_size': quote_data.get('askSize', 0),
    'order_imbalance': self._calculate_order_imbalance(depth_data),
    'liquidity_score': self._calculate_liquidity_score(depth_data),
    {            'timestamp': datetime.now(timezone.utc).isoformat() }

    return microstructure

    def _calculate_order_imbalance(self, depth_data: Dict) -> floatdef _calculate_order_imbalance(self, depth_data: Dict) -> floattotal_bid_size = sum(level.get('size', 0) for level in depth_data.get('bids', [])):
        total_ask_size = sum(level.get('size', 0) for level in depth_data.get('asks', []))
        if total_bid_size + total_ask_size == 0: return 0.0return(total_bid_size - total_ask_size) / (total_bid_size + total_ask_size):

            def _calculate_liquidity_score(self, depth_data: Dict) -> floatdef _calculate_liquidity_score(self, depth_data: Dict) -> floatif not depth_data_return 0.0:
                pass

        # Simple liquidity score based on depth
        total_depth = ()
        sum(level.get('size', 0) for level in depth_data.get('bids', [])) +
        (            sum(level.get('size', 0) for level in depth_data.get('asks', [])) )

        # Normalize to 0-100 scalereturn min(100, total_depth / 1000)

        async def close(self, async) def close(self, logger).info("Unified Data Manager closed")


        # Example usage
        async def main(self, async) def main(self, from) Redis import Cache, CacheConfig
        from data_sources.alpha_vantage_client import AlphaVantageClient
        from data_sources.fred_client import FREDClient
        from market_data.ib_data_provider importIBDataProvider
        from typing import Dict, List, Optional, Any, Tuple
        import asyncio
        import json
        import logging
        import numpy as np
        import pandas as pd
        import pickle

        manager = UnifiedDataManager()

        # Connect to all data sources
        connected = await manager.connect()
        if not connectedlogger.erro:
            pass
    r("Failed to connect to data sources")

    # Get market data for NVDAquote = await manager.get_market_data('NVDA', 'quote')

    print(f"NVDA Quote: {quote}")

    # Get historical data
    historical = await manager.get_historical_data('NVDA', '5 D', '1 hour')
    if historical is not Noneprin:
        pass
t(f"Historical data shape: {historical.shape}"):
print(historical.tail())

print(historical.tail())
news = await manager.get_news_sentiment('NVDA')
print(f"News sentiment: {news.get('trading_signal')}")

# Get economic indicators
economic = await manager.get_economic_indicators(['VIXCLS', 'DF: F'])
print(f"Market regime: {economic.get('regime')}")

# Get market microstructure
microstructure = await manager.get_market_microstructure('NVDA')
print(f"Spread: {microstructure.get('spread_bps'):.2f} bps")

# Close connections
await manager.close()
if __name__ == "__main__": asyncio.run(main()):