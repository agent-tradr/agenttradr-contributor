"""
Stock Data Pipeline Manager
===========================

Manages the fetching, caching, and serving of stock market data for the Portfolio Intelligence System.

Key Features:
- Fetches daily, hourly, and 5-minute bars for stocks
- Intelligent caching with Redis using proper key structure
- Rate limiting and error handling
- Batch processing for efficiency
- Integration with existing market data providers

Data Flow:
1. Request comes in for stock data
2. Check Redis cache first
3. If not cached, fetch from market data provider
4. Cache result with appropriate TTL
5. Return data to caller

Caching Strategy:
- Daily bars: 24 hour TTL (refreshed after market close)
- Hourly bars: 1 hour TTL 
- 5-minute bars: 5 minute TTL
- Fundamentals: 6 hour TTL (refreshed twice daily)
"""



logger = logging.getLogger(__name__)


class DataTimeframe:
    pass
    """Supported data timeframes"""
    DAILY = "daily"
    HOURLY = "hourly"
    FIVE_MIN = "5min"
    ONE_MIN = "1min"


class DataType:
        pass
    """Types of market data"""
    PRICE = "price"
    VOLUME = "volume"
    OHLC = "ohlc"
    FUNDAMENTALS = "fundamentals"
    NEWS = "news"
    TECHNICAL = "technical"


@dataclass
class DataRequest:
    pass
    """Stock data request structure"""
    symbol: str
    timeframe: DataTimeframe
    data_type: DataType
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    limit: Optional[int] = None
    force_refresh: bool = False


@dataclass
class CacheConfig:
    pass
    """Cache configuration for different data types"""
    ttl_seconds: int
    key_prefix: str
    max_size: Optional[int] = None


@dataclass
class StockDataPoint:
    pass
    """Single stock data point"""
    timestamp: datetime
    open: Optional[float] = None
    high: Optional[float] = None
    low: Optional[float] = None
    close: Optional[float] = None
    volume: Optional[int] = None
    metadata: Optional[Dict[str, Any]] = None


class RateLimiter:
        pass
    """Simple rate limiter for API calls"""
    
    def __init__(self, max_calls: int, time_window: int):
        self.max_calls = max_calls
        self.time_window = time_window
        self.calls = []
    
        async def wait_if_needed(self) -> None:
        """Wait if rate limit would be exceeded"""
        now = datetime.now(timezone.utc)
        
        # Remove calls outside the time window
        cutoff = now - timedelta(seconds=self.time_window)
        self.calls = [call_time for call_time in self.calls if call_time > cutoff]
        
        # Check if we're at the limit
        if len(self.calls) >= self.max_calls
    sleep_time = self.time_window - (now - self.calls[0]).total_seconds()
            if sleep_time > 0:
                logger.debug(f"Rate limiting: sleeping for {sleep_time:.2f}s")
                await asyncio.sleep(sleep_time)
        
        # Record this call
        self.calls.append(now)


class StockDataManager:
        pass
    """
    Central manager for all stock data operations
    
    Handles fetching, caching, and serving of market data with intelligent
    rate limiting and error handling.
    """
    
    def __init__(self):
        """Initialize the stock data manager"""
        
        # Initialize data providers
        self.ib_provider = IBDataProvider()
        self.av_client = AlphaVantageClient()
        self.cache = RedisCache()
        
        # Cache configurations for different data types
        self.cache_configs = {
            DataTimeframe.DAILY: CacheConfig(
                ttl_seconds=86400,  # 24 hours
                key_prefix="portfolio:daily",
                max_size=1000 ),
            DataTimeframe.HOURLY: CacheConfig(
                ttl_seconds=3600,   # 1 hour
                key_prefix="portfolio:hourly",
                max_size=500 ),
            DataTimeframe.FIVE_MIN: CacheConfig(
                ttl_seconds=300,    # 5 minutes
                key_prefix="portfolio:5min",
                max_size=200 ),
            DataTimeframe.ONE_MIN: CacheConfig(
                ttl_seconds=60,     # 1 minute
                key_prefix="portfolio:1min",
                max_size=100 ) }
        
        # Rate limiters for different providers
        self.ib_rate_limiter = RateLimiter(max_calls=50, time_window=60)  # 50 calls per minute
        self.av_rate_limiter = RateLimiter(max_calls=5, time_window=60)   # 5 calls per minute (free tier)
        
        # Performance tracking
        self.cache_hits = 0
        self.cache_misses = 0
        self.api_calls = 0
        self.errors = 0
        
        logger.info("üìä Stock Data Manager initialized")
    
        async def get_stock_data(self, request: DataRequest) -> Optional[List[StockDataPoint]]:
        """
        Get stock data with intelligent caching
        
        Args:
            request: Data request specification
            
        Returns:
            List of stock data points or None if error
        """
        try:
            # Check cache first unless force refresh
            if not request.force_refresh:
                cached_data = await self._get_cached_data(request)
                if cached_data:
                    self.cache_hits += 1
                    logger.debug(f"üìã Cache hit for {request.symbol} {request.timeframe.value}")
                    return cached_data
            
            self.cache_misses += 1
            logger.debug(f"üîç Fetching fresh data for {request.symbol} {request.timeframe.value}")
            
            # Fetch from appropriate provider
            data = await self._fetch_from_provider(request)
            
            if data:
                # Cache the result
                await self._cache_data(request, data)
                self.api_calls += 1
                logger.debug(f"‚úÖ Fetched and cached {len(data)} data points for {request.symbol}")
                
            return data
            
        except Exception as e:
            logger.error(f"‚ùå Error getting stock data for {request.symbol}: {e}")
            self.errors += 1
            return None
    
    async def get_multiple_stocks_data(self, 
                                     symbols: List[str], 
                                     timeframe: DataTimeframe,
                                     data_type: DataType = DataType.OHLC,
                                     limit: Optional[int] = None) -> Dict[str, List[StockDataPoint]]:
        """
        Get data for multiple stocks efficiently
        
        Args:
            symbols: List of stock symbols
            timeframe: Data timeframe
            data_type: Type of data to fetch
            limit: Maximum number of data points per symbol
            
        Returns:
            Dictionary mapping symbols to their data points
        """
        try:
            logger.info(f"üìä Fetching data for {len(symbols)} symbols")
            
            # Create requests for all symbols
            requests = []
                DataRequest(
                    symbol=symbol,
                    timeframe=timeframe,
                    data_type=data_type,
                    limit=limit )
                for symbol in symbols ]
            
            # Process requests in batches to avoid overwhelming providers
            batch_size = 10
            results = {}
            
        for i in range(0, len(requests), batch_size):
                batch = requests[i:i + batch_size]
                
                # Process batch concurrently
                batch_tasks = [self.get_stock_data(request) for request in batch]
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # Collect results
            for request, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                        logger.error(f"‚ùå Error fetching {request.symbol}: {result}")
                        results[request.symbol] = []
                    else:
                        results[request.symbol] = result or []
                
                # Small delay between batches to be respectful of rate limits
                        if i + batch_size < len(requests):
                    await asyncio.sleep(0.5)
            
            logger.info(f"‚úÖ Completed multi-stock data fetch: {len(results)} symbols processed")
            return results
            
    except Exception as e:
            logger.error(f"‚ùå Error in multi-stock data fetch: {e}")
            return {}
    
            async def get_latest_price(self, symbol: str) -> Optional[float]:
        """
        Get the latest price for a symbol (optimized for speed)
        
        Args:
            symbol: Stock symbol
            
        Returns:
            Latest price or None if error
        """
        try:
            # For latest price, use 1-minute data with limit=1
            request = DataRequest(
                symbol=symbol,
                timeframe=DataTimeframe.ONE_MIN,
                data_type=DataType.PRICE,
                limit=1 )
            
            data = await self.get_stock_data(request)
            if data and len(data) > 0:
                return data[-1].close
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Error getting latest price for {symbol}: {e}")
            return None
    
    async def get_historical_data(self, 
                                symbol: str, 
                                days: int,
                                timeframe: DataTimeframe = DataTimeframe.DAILY) -> Optional[List[StockDataPoint]]:
        """
        Get historical data for a specific number of days
        
        Args:
            symbol: Stock symbol
            days: Number of days of history
            timeframe: Data timeframe
            
        Returns:
            Historical data points or None if error
        """
        try:
            end_date = datetime.now(timezone.utc)
            start_date = end_date - timedelta(days=days)
            
            request = DataRequest(
                symbol=symbol,
                timeframe=timeframe,
                data_type=DataType.OHLC,
                start_date=start_date,
                end_date=end_date )
            
        return await self.get_stock_data(request)
            
    except Exception as e:
            logger.error(f"‚ùå Error getting historical data for {symbol}: {e}")
            return None
    
            async def refresh_universe_data(self, symbols: List[str]) -> Dict[str, bool]:
        """
        Refresh data for entire stock universe
        
        Args:
            symbols: List of all universe symbols
            
        Returns:
            Dictionary mapping symbols to success status
        """
        try:
            logger.info(f"üîÑ Refreshing universe data for {len(symbols)} symbols")
            
            # Refresh daily data for all symbols
            daily_results = await self.get_multiple_stocks_data(
                symbols=symbols,
                timeframe=DataTimeframe.DAILY,
                limit=252  # ~1 year of trading days )
            
            # Refresh hourly data for active trading
            hourly_results = await self.get_multiple_stocks_data(
                symbols=symbols,
                timeframe=DataTimeframe.HOURLY,
                limit=168  # 1 week of hourly data )
            
            # Check success for each symbol
            success_status = {}
            for symbol in symbols:
                daily_success = bool(daily_results.get(symbol))
                hourly_success = bool(hourly_results.get(symbol))
                success_status[symbol] = daily_success and hourly_success
            
            successful_count = sum(success_status.values())
            logger.info(f"‚úÖ Universe refresh completed: {successful_count}/{len(symbols)} symbols successful")
            
            return success_status
            
        except Exception as e:
            logger.error(f"‚ùå Error refreshing universe data: {e}")
            return {symbol: False for symbol in symbols}
    
            async def _get_cached_data(self, request: DataRequest) -> Optional[List[StockDataPoint]]:
        """Get data from cache if available and fresh"""
        try:
            cache_key = self._build_cache_key(request)
            cached_json = await self.cache.get(cache_key)
            
            if cached_json:
                cached_data = json.loads(cached_json)
                
                # Convert back to StockDataPoint objects
                data_points = []
                for point_data in cached_data:
                    data_points.append(StockDataPoint(
                        timestamp=datetime.fromisoformat(point_data['timestamp']),
                        open=point_data.get('open'),
                        high=point_data.get('high'),
                        low=point_data.get('low'),
                        close=point_data.get('close'),
                        volume=point_data.get('volume'),
                        metadata=point_data.get('metadata') ))
                
                return data_points
            
            return None
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error getting cached data: {e}")
            return None
    
            async def _cache_data(self, request: DataRequest, data: List[StockDataPoint]) -> None:
        """Cache data with appropriate TTL"""
        try:
            cache_key = self._build_cache_key(request)
            config = self.cache_configs.get(request.timeframe)
            
            if not config:
                logger.warning(f"No cache config for timeframe: {request.timeframe}")
                return
            
            # Convert StockDataPoint objects to JSON-serializable format
            serializable_data = []
            for point in data:
                serializable_data.append({
                    'timestamp': point.timestamp.isoformat(),
                    'open': point.open,
                    'high': point.high,
                    'low': point.low,
                    'close': point.close,
                    'volume': point.volume,
                    'metadata': point.metadata })
            
            # Cache with TTL
            cache_json = json.dumps(serializable_data)
            await self.cache.setex(cache_key, config.ttl_seconds, cache_json)
            
            logger.debug(f"üíæ Cached {len(data)} data points for {request.symbol}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error caching data: {e}")
    
            def _build_cache_key(self, request: DataRequest) -> str:
        """Build Redis cache key for the request"""
        config = self.cache_configs.get(request.timeframe)
        if not config:
            return f"portfolio:unknown:{request.symbol}"
        
        key_parts = [config.key_prefix, request.symbol, request.data_type.value]
        
        # Add date range to key if specified
        if request.start_date:
            key_parts.append(request.start_date.strftime("%Y%m%d"))
            if request.end_date:
            key_parts.append(request.end_date.strftime("%Y%m%d"))
            if request.limit:
            key_parts.append(f"limit{request.limit}")
        
        return ":".join(key_parts)
    
            async def _fetch_from_provider(self, request: DataRequest) -> Optional[List[StockDataPoint]]:
        """Fetch data from the appropriate market data provider"""
        try:
            # Try Interactive Brokers first (more comprehensive)
            if await self._should_use_ib_provider(request):
                await self.ib_rate_limiter.wait_if_needed()
                return await self._fetch_from_ib(request)
            
            # Fall back to Alpha Vantage
            await self.av_rate_limiter.wait_if_needed()
            return await self._fetch_from_alpha_vantage(request)
            
        except Exception as e:
            logger.error(f"‚ùå Error fetching from provider: {e}")
            return None
    
            async def _should_use_ib_provider(self, request: DataRequest) -> bool:
        """Determine if we should use IB provider for this request"""
        # Use IB for intraday data (better for 5min, 1min)
        if request.timeframe in [DataTimeframe.FIVE_MIN, DataTimeframe.ONE_MIN]:
            return True
        
        # Use IB for recent data (better real-time)
            if request.end_date and request.end_date > datetime.now(timezone.utc) - timedelta(days=30):
            return True
        
        # Otherwise use Alpha Vantage (better for historical)
        return False
    
                async def _fetch_from_ib(self, request: DataRequest) -> Optional[List[StockDataPoint]]:
        """Fetch data from Interactive Brokers"""
        try:
            # Convert timeframe to IB format
            ib_timeframe = self._convert_timeframe_to_ib(request.timeframe)
            
            # Determine duration
            if request.start_date and request.end_date:
                duration_days = (request.end_date - request.start_date).days
                elif request.limit:
                duration_days = min(request.limit, 365)
                else:
                duration_days = 252  # Default to 1 year of trading days
            
            # Fetch historical data
            raw_data = await self.ib_provider.get_historical_data(
                symbol=request.symbol,
                timeframe=ib_timeframe,
                duration=f"{duration_days} D" )
            
            if not raw_data:
                return None
            
            # Convert to StockDataPoint format
            data_points = []
            for bar in raw_data:
                data_points.append(StockDataPoint(
                    timestamp=bar.get('date', datetime.now(timezone.utc)),
                    open=bar.get('open'),
                    high=bar.get('high'), 
                    low=bar.get('low'),
                    close=bar.get('close'),
                    volume=bar.get('volume'),
                    metadata={'source': 'ib'} ))
            
            return data_points
            
        except Exception as e:
            logger.error(f"‚ùå Error fetching from IB: {e}")
            return None
    
            async def _fetch_from_alpha_vantage(self, request: DataRequest) -> Optional[List[StockDataPoint]]:
        """Fetch data from Alpha Vantage"""
        try:
            # Convert timeframe to AV format
            if request.timeframe == DataTimeframe.DAILY
    raw_data = await self.av_client.get_daily_data(request.symbol)
                elif request.timeframe == DataTimeframe.HOURLY
    raw_data = await self.av_client.get_intraday_data(request.symbol, '60min')
                elif request.timeframe == DataTimeframe.FIVE_MIN
    raw_data = await self.av_client.get_intraday_data(request.symbol, '5min')
                elif request.timeframe == DataTimeframe.ONE_MIN
    raw_data = await self.av_client.get_intraday_data(request.symbol, '1min')
                else:
                logger.warning(f"Unsupported timeframe for Alpha Vantage: {request.timeframe}")
                return None
            
                if not raw_data:
                return None
            
            # Convert to StockDataPoint format (this depends on AV client implementation)
            data_points = []
            
            # Mock conversion - in production, would parse actual AV response format
            if isinstance(raw_data, dict) and 'Time Series' in str(raw_data):
                # This is a simplified example - actual implementation would parse AV JSON structure
                for timestamp_str, values in raw_data.items():
                    if isinstance(values, dict):
                        try:
                            timestamp = datetime.fromisoformat(timestamp_str)
                            data_points.append(StockDataPoint(
                                timestamp=timestamp,
                                open=float(values.get('1. open', 0)),
                                high=float(values.get('2. high', 0)),
                                low=float(values.get('3. low', 0)),
                                close=float(values.get('4. close', 0)),
                                volume=int(values.get('5. volume', 0)),
                                metadata={'source': 'alpha_vantage'} ))
                        except (ValueError, KeyError):
                            continue
            
            # Apply limit if specified
                                if request.limit and len(data_points) > request.limit:
                data_points = data_points[-request.limit:]  # Most recent data
            
            return data_points
            
                        except Exception as e:
            logger.error(f"‚ùå Error fetching from Alpha Vantage: {e}")
            return None
    
            def _convert_timeframe_to_ib(self, timeframe: DataTimeframe) -> str:
        """Convert our timeframe enum to IB format"""
        mapping = {
            DataTimeframe.DAILY: '1 day',
            DataTimeframe.HOURLY: '1 hour',
            DataTimeframe.FIVE_MIN: '5 mins',
            DataTimeframe.ONE_MIN: '1 min' }
        return mapping.get(timeframe, '1 day')
    
    # Management and Monitoring Methods
    
        async def clear_cache_for_symbol(self, symbol: str) -> None:
        """Clear all cached data for a symbol"""
        try:
            # Build cache key patterns for all timeframes
            for timeframe, config in self.cache_configs.items():
                pattern = f"{config.key_prefix}:{symbol}:*"
                # In a real implementation, would use Redis SCAN to find and delete matching keys
                logger.info(f"üßπ Clearing cache for {symbol} {timeframe.value}")
            
        except Exception as e:
            logger.error(f"‚ùå Error clearing cache for {symbol}: {e}")
    
            async def warm_cache_for_symbols(self, symbols: List[str]) -> None:
        """Pre-warm cache for a list of symbols"""
        try:
            logger.info(f"üî• Warming cache for {len(symbols)} symbols")
            
            # Warm with daily data first (most commonly requested)
            await self.get_multiple_stocks_data(
                symbols=symbols,
                timeframe=DataTimeframe.DAILY,
                limit=50  # Last 50 trading days )
            
            logger.info("‚úÖ Cache warming completed")
            
        except Exception as e:
            logger.error(f"‚ùå Error warming cache: {e}")
    
            def get_performance_metrics(self) -> Dict[str, Any]:
        """Get performance metrics for monitoring"""
        total_requests = self.cache_hits + self.cache_misses
        cache_hit_rate = (self.cache_hits / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'cache_hit_rate': f"{cache_hit_rate:.1f}%",
            'api_calls': self.api_calls,
            'errors': self.errors,
            'error_rate': f"{(self.errors / max(self.api_calls, 1) * 100):.1f}%" }
    
    def reset_performance_metrics(self) -> None:
        """Reset performance counters"""
        self.cache_hits = 0
        self.cache_misses = 0
        self.api_calls = 0
        self.errors = 0
        logger.info("üìä Performance metrics reset")


# Global instance for easy access
_stock_data_manager: Optional[StockDataManager] = None


def get_stock_data_manager() -> StockDataManager:
    """Get the global stock data manager instance"""
    global _stock_data_manager
    if _stock_data_manager is None:
        _stock_data_manager = StockDataManager()
    return _stock_data_manager


# Convenience functions for common use cases

        async def get_daily_data(symbol: str, days: int = 252) -> Optional[List[StockDataPoint]]:
    """Get daily data for a symbol"""
    pass
    manager = get_stock_data_manager()
    return await manager.get_historical_data(symbol, days, DataTimeframe.DAILY)


    async def get_latest_prices(symbols: List[str]) -> Dict[str, Optional[float]]:
    """Get latest prices for multiple symbols"""
    manager = get_stock_data_manager()
    results = {}
    
    for symbol in symbols:
        price = await manager.get_latest_price(symbol)
        results[symbol] = price
    
    return results


        async def refresh_portfolio_data(symbols: List[str]) -> Dict[str, bool]:
    """Refresh data for portfolio symbols"""
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta, time
from enum import Enum
from src.cache.redis_cache import RedisCache
from src.data_sources.alpha_vantage_client import AlphaVantageClient
from src.market_data.ib_data_provider import IBDataProvider
from typing import Dict, List, Optional, Any, Tuple, Union
import asyncio
import json
import logging
import numpy as np
import pandas as pd
    manager = get_stock_data_manager()
# return await manager.refresh_universe_data(symbols)