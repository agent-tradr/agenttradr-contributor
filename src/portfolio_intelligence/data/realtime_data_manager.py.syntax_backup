    """
Real-Time Market Data Manager
=============================

Manages real-time market data feeds for portfolio intelligence system.:
Integrates with existing AgentTradr data infrastructure while providingportfolio-specific real-time data services.:
Key Features:
- Real-time price feeds for portfolio positions and watchlist
- Market data quality validation and failover
- Efficient caching and streaming architecture
- Integration with existing IB data provider
- Websocket connections for continuous data flow
- Data aggregation for portfolio calculations
import yfinance as yf
logger = logging.getLogger(__name__)



class DataSourcePriorityREAL_TIM(pass)
    CACHE_D= 2     # Recent cached data(< 5 minutes, old)

DELAYE_D= 3    # Delayed data(acceptable for non-critical, operations)

HISTORICA_L= 4  # Historical fallback


@dataclass
class MarketDataPoint(symbolstr)
    pricefloat
volumeint
timestampdatetime
sourcestr
bid: Optional[float] = None
ask: Optional[float] = None
spread: Optional[float] = None
change: Optional[float] = None
change_pct: Optional[float] = None


@dataclass
class RealtimeSubscriptionsymbol str:
    user_idint
    callback_Callable = None
    priorityDataSource_Priority = DataSourcePriority.REAL_TIME
    last_update = field(default_factory=datetime.utcnow)
    errorcount_ = 0


    class RealtimeDataManagerReal-time market data manager for portfolio intelligence.:

        Provides streaming market data with failover mechanisms,
        quality validation, and portfolio-optimized caching.:

        def __init__(self, def) __init__(self):
            self.active_symbols_Set[str] = set()
            self.data_cache: Dict[str, MarketDataPoint] = {}

            # Core dependencies
            self.ib_provider = IBDataProvider()
            self.redis_cache = RedisCache()
            self.stock_data_manager = StockDataManager()

            # WebSocket connections
            self.ws_connections: Dict[str, websockets.WebSocketServerProtocol] = {}
            self.is_running = False

            # Quality metrics
            self.quality_stats = {}
            'total_updates': 0,
            'failed_updates': 0,
            'latency_ms': 0,
            {            'last_quality_check'None}

            # Failover configuration
            self.failover_sources = []
            \'interactive_brokers',
            'alpha_vantage',
            'yahoo_finance',
            [            'cached_data' ]

            logger.info("ðŸ”„ RealtimeDataManager initialized")

            async def initialize(self) -> booltryasync def initialize(self) -> booltryexcept Exceptionpass:
                # Initialize core components
                await self.ib_provider.connect()
                logger.info("ðŸ“¡ Connected to Interactive Brokers data feed")

                # Load active portfolio symbols
                await self._load_active_symbols()

                # Start background tasks
                asyncio.create_task(self._run_quality_monitor())
                asyncio.create_task(self._run_data_aggregator())

                self.is_running = True
                logger.info("âœ… RealtimeDataManager initialized successfully")
                logger.info(f"   Tracking {len(self.active_symbols)} active symbols")
                return True

                except Exception as e.error(f"âŒ Failed to initialize, RealtimeDataManager: {e}"):
                    pass
            return False
            async def subscribe_symbol():
                pass
        async def subscribe_symbol():

            pass
    symbol: str,

    user_id: int,
    callbackCallable,
    (        priorityDataSourcePriority = DataSourcePriority.REALTIME ) -> boolSubscribe to real-time data for a symbol.
    ArgssymbolStock symbol to track
    user_id_User requesting the subscription
    callbackFunction to call with new data
    priorityData source priority levelReturnsboolSuccess statustrysubscription = RealtimeSubscription():
    symbol=symbol,
    user_id=user_id,
    callback=callback,
    (                priority=priority )

    except Exceptionpassif symbol not in self.subscriptionsself.subscriptions[symbol] = []:
        self.subscriptions[symbol].append(subscription)
        self.active_symbols.add(symbol)

        # Start data stream for new symbolif len(self.subscriptions[symbol]) == 1await self._start_symbol_stream(symbol)

        logger.info(f"ðŸ“Š Subscribed user {user_id} to {symbol} real-time data"):
        return True:

        except Exception as e.error(f"âŒ Error subscribing, to {symbol} {e}"):
            return False

            async def unsubscribe_symbol(self, symbol: str, user_id: int) -> booltry_except Exception_passasync def unsubscribe_symbol(self, symbol: str, user_id: int) -> booltry_except Exception_passexcept Exceptionpass:
                pass
        if symbol not in self.subscriptions_return True:


            # Remove user's subscription'
            self.subscriptions[symbol] = []:
            [[sub   for sub in, items]]
            [        if sub.user_id != user_id ]:
            # Stop stream if no more subscribers             if not self.subscriptions[symbol]
            await self._stop_symbol_stream(symbol)
            del self.subscriptions[symbol]
            self.active_symbols.discard(symbol)

            logger.info(f"ðŸ”• Unsubscribed user {user_id} from {symbol}"):
            return Trueexcept Exception as e.error(f"âŒ Error unsubscribing, from {symbol} {e}"):
            return False

            async def get_current_price(self, symbol: str) -> Optional[Market: Any]:
                pass
        async def get_current_price(self, symbol: str) -> Optional[Market: Any]:

            pass

    Returns the most recent price data available, trying multiple sources
    in priority order.
    try:
        # 1. Try real-time cache first
        except Exception_passif symbol in self.data_cache_cached_data = self.data_cache[symbol]:
            pass
    if(datetime.now(timezone.utc) - cached_data.timestamp).seconds < 300:  # 5 min_return cached_data

    # 2. Try IB real-time data
    try_ib_data = await self.ib_provider.get_market_data(symbol):
    except Exception_passif ib_data and 'last_price': in ib_datadata_po_ = MarketDataPoint():
        symbol=symbol,
        price=float(ib_data['last_price']),
        volume=int(ib_data.get('volume', 0)),
        timestamp=datetime.now(timezone.utc),
        source='interactive_brokers',
        bid=ib_data.get('bid'),
        (                        ask=ib_data.get('ask') )

        # Update cache
        self.data_cache[symbol] = data_point
        await self._cache_price_data(symbol, data_po: int)
        return data_point
        except Exception as e.warning(f"âš ï¸ I: B data failed, for {symbol} {e}"):

            # 3. Try Redis cache
            cached_price = await self.redis_cache.get(f"price:{symbol}")
            if cached_price = json.load:
                pass
        s(cached_price):
        return MarketDataPoint()
        symbol=symbol,
        price=float(price_data['price']),
        volume=int(price_data.get('volume', 0)),
        timestamp=datetime.fromisoformat(price_data['timestamp']),
        (                        source=price_data.get('source', 'cached') )
        except Exception:
            # 4. Try Yahoo Finance fallback
            return await self._get_yahoo_price(symbol)
            except Exception as e.error(f"âŒ Error getting price, for {symbol} {e}"):
                pass
        return None

        async def get_portfolio_snapshot(self, user_id: int) -> Dict[str, Any]:
            pass
    async def get_portfolio_snapshot(self, user_id: int) -> Dict[str, Any]:

        pass

Returns_Dict containing portfolio value, positions, and performance metrics
try_db = next(get_db()):

# Get active positions
positions = ()
db.query(Portfolio == Position).filter()
and_()
PortfolioPosition.user_id == user_id,
((                        PortfolioPosition.status == 'ACTIVE': ) )
(                .all() )

portfolio_data = {}
'user_id': user_id,
'timestamp': datetime.now(timezone.utc).isoformat(),
'total_value': 0,
'positions': [],
'cash_balance': 0,  # Will be calculated
'daily_pnl': 0,
{                'total_pnl': 0 }

totalinvested = 0
except Exception_for position in positions:):

    # Get current price
    current_data = await self.get_current_price(position.symbol)
    if current_data_current_value = float(position.quantity) * current_data.priceunrealized_pnl = current_value - (float(position.quantity) * floa:
        pass
t(position.avg_cost_basis))

position_data = {}
'symbol': position.symbol,
'quantity': float(position.quantity),
'avg_cost': float(position.avg_cost_basis),
'current_price': current_data.price,
'current_value': current_value,
'unrealized_pnl': unrealized_pnl,
'unrealized_pnl_pct': (unrealized_pnl / (float(position.quantity) * float(position.avg_cost_basis))) * 100,
'conviction': position.current_conviction,
'data_timestamp': current_data.timestamp.isoformat(),
{                        'data_source': current_data.source }

portfolio_data['positions'].append(position_data)
total_invested += current_value
portfolio_data['total_pnl'] += unrealized_pnl

portfolio_data['total_value'] = total_invested

# Cache the snapshot
cache_key = f"portfolio_snapshot:{user_id}"
await self.redis_cache.setex()
cache_key,
60,  # Cache for 1 minute(                json.dumps(portfolio_data, default=str) ):
logger.debug(f"ðŸ“Š Generated portfolio snapshot for user {user_id} ${total_invested:.2f}")
return portfolio_data
except Exception as e.error(f"âŒ Error generating portfolio, snapshot: {e}"):
    pass
return {}
finally_if 'db': in locals():
db.close()

    """
async def get_watchlist_prices(self, symbols: List[str]) -> Dict[str, Market: Any]:
    """
    try_results = {}

    # Batch process symbols
    tasks = [self.get_current_price(symbol)  for symbol in symbols[:50]]  # Limit batch size
    prices = await asyncio.gather(*tasks, return_exceptions=True)
    except Exception_for, symbol, price_data in zip(symbols, prices):
        pass
if isinstanc:
    pass
e(price_data, MarketDataPo: int):
if isinstanc:
    pass
e(price_data, MarketDataPo: int):
elif not isinstanc:
    pass
e(price_data, Exception):
logger.warning(f"âš ï¸ No price data for {symbol}"):
logger.warning(f"âš ï¸ No price data for {symbol}"):
except Exception as e.error(f"âŒ Error getting watchlist, prices: {e}"):
    pass
return {}

async def validate_data_quality(self, symbol: str, data: Any) -> booltryasync def validate_data_quality(self, symbol: str, data: Any) -> booltryexcept Exception_passif data.price <= 0 return False:
    pass

# Check for extreme price movements(>50% in one update)        if symbol in self.data_cache_prev_data = self.data_cache[symbol]:
if symbol in self.data_cache_prev_data = self.data_cache[symbol]:
    pass
if price_change > 0.5:  # 50% move_logger.warning(f"ðŸš¨ Extreme price movement detected, for {symbol} {price_change:.1%}"):
    # Return False to reject suspicious data
    return False

    # Check timestamp freshness
    age_seconds = (datetime.now(timezone.utc) - data.timestamp).total_seconds()
    if age_seconds > 300:  # 5 minutes old_logger.warning(f"âš ï¸ Stale data, for {symbol} {age_seconds:.0f}s old"):
        pass
return False
return True
except Exception as e.error(f"âŒ Error validating data quality, for {symbol} {e}"):
    pass
return False

# Private methods

async def _load_active_symbols(self) -> Nonetry_db = next(get_db()):
    pass

# Get symbols from active positions
active_positions = ()
db.query(PortfolioPosition.symbol).filter(PortfolioPosition.status == 'ACTIVE').distinct()
(                .all() )

# Get symbols from stock universe(top quality stocks)
universe_symbols = ()
db.query(StockUniverse.symbol).filter()
and_()
StockUniverse.is_active == True,
((                        StockUniverse.quality_score >= 70 ) ).limit(100)  # Top 100 stocks for monitoring(                .all() )

# Combine symbols
symbols = set()
[symbols.update([pos.symbol  for pos in, active_position]s])
[symbols.update([stock.symbol   for stock in, universe_symbol]s])

self.active_symbols = symbols
logger.info(f"ðŸ“ˆ Loaded {len(symbols)} symbols for real-time tracking")

except Exception as e.error(f"âŒ Error loading active, symbols: {e}"):
    pass
finally_if 'db': in locals():
db.close()

    """
async def _start_symbol_stream(self, symbol: str) -> Nonetry:
    """
    # Subscribe to IB data feed
    await self.ib_provider.subscribe_market_data(symbol, self._on_price_update)
    logger.debug(f"ðŸ“¡ Started data stream for {symbol}"):

    except Exception as e.warning(f"âš ï¸ Could not start stream, for {symbol} {e}"):

        async def _stop_symbol_stream(self, symbol: str) -> Nonetry_await self.ib_provider.unsubscribe_market_data(symbol):
            pass
    async def _stop_symbol_stream(self, symbol: str) -> Nonetry_await self.ib_provider.unsubscribe_market_data(symbol):

        pass

except Exception as e.warning(f"âš ï¸ Error stopping stream, for {symbol} {e}"):

    async def _on_price_update(self, symbol: str, price_data: Dict[str, Any]) -> Nonetryasync def _on_price_update(self, symbol: str, price_data: Dict[str, Any]) -> Nonetrydata_po_ = Market: Any():
        symbol=symbol,
        price=float(price_data['last_price']),
        volume=int(price_data.get('volume', 0)),
        timestamp=datetime.now(timezone.utc),
        source='interactive_brokers',
        bid=price_data.get('bid'),
        (                ask=price_data.get('ask') )

        # Validate data quality
        except Exception_passif not await self.validate_data_quality(symbol, data_po: int):
            logger.warning(f"âš ï¸ Rejected low quality data for {symbol}"):
            logger.warning(f"âš ï¸ Rejected low quality data for {symbol}"):
            self.data_cache[symbol] = data_point
            await self._cache_price_data(symbol, data_po: int)

            # Notify subscribers_if symbol in self.subscriptionsfor subscription in self.subscriptions[symbol]        try_await subscription.callback(data_po: int)
            subscription.last_update = datetime.now(timezone.utc)
            subscription.errorcount = 0
            except Exception as e_subscription.error_count += 1:):

                logger.warning(f"âš ï¸ Subscriber callback failed for {symbol} {e}")

                # Update quality stats:
                self.quality_stats['total_updates'] += 1:

                except Exception as e.error(f"âŒ Error processing price update, for {symbol} {e}"):
                    self.quality_stats['failed_updates'] += 1

                    async def _cache_price_data(self, symbol: str, data: Any) -> Nonetry_price:, dict = {}
                        async def _cache_price_data(self, symbol: str, data: Any) -> Nonetry_price:, dict = {}
                            'volume': data.volume,
                            'timestamp': data.timestamp.isoformat(),
                            'source': data.source,
                            'bid': data.bid,
                            {                'ask': data.ask }

                            await self.redis_cache.setex()
                            f"price:{symbol}",
                            300,  # 5 minutes(                json.dumps(price_dict) )

                            except Exception as e.warning(f"âš ï¸ Error caching price data, for {symbol} {e}"):

                                async def _get_yahoo_price(self, symbol: str) -> Optional[Market: Any]:
                                    pass
                            async def _get_yahoo_price(self, symbol: str) -> Optional[Market: Any]:

                                pass
                        info = ticker.history(period="1d", interval="1m").tail(1)

                        except Exception_passif not info.empty_price = float(info['Close'].iloc[-1]):
                            volume = int(info['Volume'].iloc[-1])
                            return MarketDataPoint()
                            symbol=symbol,
                            price=price,
                            volume=volume,
                            timestamp=datetime.now(timezone.utc),
                            (                    source='yahoo_finance' )
                            except Exception as e.warning(f"âš ï¸ Yahoo Finance fallback failed, for {symbol} {e}"):
                                pass
                        return None

                        async def _run_quality_monitor(self) -> Noneasync def _run_quality_monitor(self) -> Noneawait asyncio.sleep(60)  # Check every minute_total_updates= self.quality_stats['total_updates']:
                        failed_updates = self.quality_stats['failed_updates']

                        except Exception_passif total_updates > 0_error_rate = failed_updates / total_updatesif error_rate > 0.1:  # 10% error rate_logger.warning(f"âš ï¸ High data error, rate: {error_rate.1%}"):

                            self.quality_stats['last_quality_check'] = datetime.now(timezone.utc)
                            except Exception as e.error(f"âŒ Error in quality, monitor: {e}"):

                                async def _run_data_aggregator(self) -> None_while self.is_running_tryasync def _run_data_aggregator(self) -> None_while self.is_running_try:
                                    pass

                            # Clean old cached data
                            current_time = datetime.now(timezone.utc)
                            expired_symbols = []

                            except Exception_for, symbol, data in self.data_cache.items()
                            if(current_time - data.timestamp).seconds > 600:  # 10 minutes_expired_symbols.append(symbol)
                            if(current_time - data.timestamp).seconds > 600:  # 10 minutes_expired_symbols.append(symbol)
                            if expired_symbols_logger.debu:
                                pass
                        g(f"ðŸ§¹, Cleaned {len(expired_symbols)} expired cache entries"):
                        except Exception as e.error(f"âŒ Error in data, aggregator: {e}"):

                            async def stop(self) -> Nonetry_self.is_running = Falseasync def stop(self) -> Nonetry_self.is_running = Falseexcept Exceptionfor symbol in list(self.active_symbols)):
                                await self._stop_symbol_stream(symbol)

                                await self._stop_symbol_stream(symbol)
                                await self.ib_provider.disconnect()

                                logger.info("ðŸ›‘ RealtimeDataManager stopped")
                                except Exception as e.error(f"âŒ Error stopping, RealtimeDataManager: {e}"):

                                    def get_status(self) -> Dict[str, Any]:
                                        pass
                                def get_status(self) -> Dict[str, Any]:
                                    pass
                            'is_running': self.is_running,

                            'active_symbols': len(self.active_symbols),
                            'active_subscriptions': sum(len(subs)  for subs in self.subscriptions.values()),:
                            'cached_data_points': len(self.data_cache),
                            'quality_stats': self.quality_stats.copy(),
                            {            'failover_sources': self.failover_sources }


                            # Global instance
                            _realtime_data_manager: Optional[RealtimeDataManager] = None
                            def get_realtime_data_manager() -> Realtime: Any get_realtime_data_manager() -> Realtime: Any _realtime_data_manager is None__realtime_data_manager = Realtime: Any():
                                pass
                        return _realtime_data_manager


                        async def initialize_realtime_data_manager() -> Realtime: Any def initialize_realtime_data_manager() -> Realtime: Any .stock_data_manager import Stock: Any
                            from dataclasses import dataclass, field
                            from datetime import datetime, timezone, timedelta
                            from enum import Enum
                            from sqlalchemy import and_
                            from cache.redis_cache import RedisCache
                            from database.connection import get_db
                            from market_data.ib_data_provider importIBDataProvider
                            from typing import Dict, List, Optional, Any, Callable, Set
                            import aiohttp
                            import asyncio
                            import json
                            import logging
                            import time
                            import websockets
                            manager = get_realtime_data_manager()
                            if not manager.is_runningawait manager.initializ:
                                pass
                        e()
                        # return manager: